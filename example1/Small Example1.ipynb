{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bc013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/makayla/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install crosslingual-coreference\n",
    "#!python -m ipykernel install --user --name spacey_pipeline2\n",
    "import spacy\n",
    "import crosslingual_coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b03ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'head': 'Jennifer Anne Doudna', 'type': 'date of birth', 'tail': 'February 19, 1964'}, {'head': 'Jennifer Anne Doudna', 'type': 'award received', 'tail': 'Nobel Prize in Chemistry'}, {'head': 'Emmanuelle Charpentier', 'type': 'award received', 'tail': 'Nobel Prize in Chemistry'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "sent = '<s><triplet> Jennifer Anne Doudna <subj> February 19, 1964 <obj> date of birth <subj> Nobel Prize in Chemistry <obj> award received <triplet> Emmanuelle Charpentier <subj> Nobel Prize in Chemistry <obj> award received</s>'\n",
    "print(extract_triplets(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a23f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language\n",
    "from typing import List\n",
    "\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def call_wiki_api(item):\n",
    "  try:\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except:\n",
    "    return 'id-less'\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"rebel\",\n",
    "    requires=[\"doc.sents\"],\n",
    "    assigns=[\"doc._.rel\"],\n",
    "    default_config={\n",
    "        \"model_name\": \"Babelscape/rebel-large\",\n",
    "        \"device\": -1,\n",
    "    },\n",
    ")\n",
    "class RebelComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp,\n",
    "        name,\n",
    "        model_name: str,\n",
    "        device: int,\n",
    "    ):\n",
    "        assert model_name is not None, \"\"\n",
    "        self.triplet_extractor = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n",
    "        self.entity_mapping = {}\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"rel\"):\n",
    "          Doc.set_extension(\"rel\", default={})\n",
    "\n",
    "    def get_wiki_id(self, item: str):\n",
    "        mapping = self.entity_mapping.get(item)\n",
    "        if mapping:\n",
    "          return mapping\n",
    "        else:\n",
    "          res = call_wiki_api(item)\n",
    "          self.entity_mapping[item] = res\n",
    "          return res\n",
    "\n",
    "    \n",
    "    def _generate_triplets(self, sent: Span) -> List[dict]:\n",
    "        output_ids = self.triplet_extractor(sent.text, return_tensors=True, return_text=False)[0][\"generated_token_ids\"][\"output_ids\"]\n",
    "        extracted_text = self.triplet_extractor.tokenizer.batch_decode(output_ids[0])\n",
    "        extracted_triplets = extract_triplets(extracted_text[0])\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        for triplet in triplets:\n",
    "            # Remove self-loops (relationships that start and end at the entity)\n",
    "            if triplet['head'] == triplet['tail']:\n",
    "                continue\n",
    "\n",
    "            # Use regex to search for entities\n",
    "            head_span = re.search(triplet[\"head\"], doc.text)\n",
    "            tail_span = re.search(triplet[\"tail\"], doc.text)\n",
    "\n",
    "            # Skip the relation if both head and tail entities are not present in the text\n",
    "            # Sometimes the Rebel model hallucinates some entities\n",
    "            if not head_span or not tail_span:\n",
    "                continue\n",
    "\n",
    "            index = hashlib.sha1(\"\".join([triplet['head'], triplet['tail'], triplet['type']]).encode('utf-8')).hexdigest()\n",
    "            if index not in doc._.rel:\n",
    "                # Get wiki ids and store results\n",
    "                doc._.rel[index] = {\"relation\": triplet[\"type\"], \"head_span\": {'text': triplet['head'], 'id': self.get_wiki_id(triplet['head'])}, \"tail_span\": {'text': triplet['tail'], 'id': self.get_wiki_id(triplet['tail'])}}\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        for sent in doc.sents:\n",
    "            sentence_triplets = self._generate_triplets(sent)\n",
    "            self.set_annotations(doc, sentence_triplets)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fd036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415c6edc591c4c258f304653bd0eaa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820100b3d3774566ac05f07516d8deb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f096fb18ff4b5aa3cb27e57b36e93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598d477f17d740449f7c847aa811a196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84034f0183bf4e26851830f4d5a648d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b296139e83014edcb00c4f43f7573c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9691b69b8d84979bea8e7d8ddc52db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/344 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c4ef7124bf4444b52c6dc214ac85c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<__main__.RebelComponent at 0x7f82c0d4bb50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU\n",
    "\n",
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})\n",
    "\n",
    "# Define rel extraction model\n",
    "\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c174a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385d9ac30c35635c7e36a1636ded4a091f830cf3: {'relation': 'country of citizenship', 'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'}, 'tail_span': {'text': 'Germany', 'id': 'Q183'}}\n",
      "471a35571b66cc8e1e3415e27f7086505310efc6: {'relation': 'employer', 'head_span': {'text': 'Christian Drosten', 'id': 'Q1079331'}, 'tail_span': {'text': 'Google', 'id': 'Q95'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"Christian Drosten works in Germany. He likes to work for Google.\"\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "\n",
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b09f32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Define Neo4j connection\n",
    "host = 'bolt://127.0.0.1:7687'\n",
    "user = 'neo4j'\n",
    "password = '12345678'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb0b2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j.api.ServerInfo at 0x7f828c24eeb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get_server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f88283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (h:Entity {id: CASE WHEN NOT row.head_span.id = 'id-less' THEN row.head_span.id ELSE row.head_span.text END})\n",
    "ON CREATE SET h.text = row.head_span.text\n",
    "MERGE (t:Entity {id: CASE WHEN NOT row.tail_span.id = 'id-less' THEN row.tail_span.id ELSE row.tail_span.text END})\n",
    "ON CREATE SET t.text = row.tail_span.text\n",
    "WITH row, h, t\n",
    "CALL apoc.merge.relationship(h, toUpper(replace(row.relation,' ', '_')),\n",
    "  {file_id: row.file_id},\n",
    "  {},\n",
    "  t,\n",
    "  {}\n",
    ")\n",
    "YIELD rel\n",
    "RETURN distinct 'done' AS result;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_query(query, params={}):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, params)\n",
    "        return pd.DataFrame([r.values() for r in result], columns=result.keys())\n",
    "\n",
    "import json\n",
    "def store_content(file):\n",
    "    #try:\n",
    "    file_id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    f = open(file)\n",
    "    doc = json.load(f)\n",
    "    input_text = doc[\"text\"]\n",
    "    print(input_text[:100])\n",
    "    coref_text = coref(input_text)._.resolved_text\n",
    "    doc = rel_ext(coref_text)\n",
    "    params = [rel_dict for value, rel_dict in doc._.rel.items()]\n",
    "    for p in params:\n",
    "        p['file_id']=file_id\n",
    "    run_query(import_query, {'data': params})\n",
    "    #except Exception as e:\n",
    "    #  print(f\"Couldn't parse text for {page} due to {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e54805f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['domains/d1.txt.json',\n",
       " 'domains/d4.txt.json',\n",
       " 'domains/d5.txt.json',\n",
       " 'domains/d3.txt.json',\n",
       " 'domains/d6.txt.json',\n",
       " 'domains/d2.txt.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "DIR=\"domains\"\n",
    "files = glob.glob(DIR + '/*.json')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01c91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing domains/d1.txt.json\n",
      "Psychological Attributes\n",
      "\n",
      "Many factors determine how pain signals from the periphery are transforme\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makayla/Documents/cal-poly-classes/dist-systs/csc369-mind-over-matter/spacey_pipeline/lib/python3.9/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing domains/d4.txt.json\n",
      "Systemic Attributes\n",
      "\n",
      "One’s pain experience can be significantly influenced by systemic factors and c\n",
      "Parsing domains/d5.txt.json\n",
      "Familial Attributes\n",
      "\n",
      "Genetic and environmental conditions shared in families may contribute to the d\n",
      "Parsing domains/d3.txt.json\n",
      "Societal Attributes\n",
      "\n",
      "The chronic pain experience can be influenced by social context: the quality of\n",
      "Parsing domains/d6.txt.json\n",
      "Central Schema Attributes\n",
      "\n",
      "Many factors determine how pain signals from the periphery are transforme\n",
      "Parsing domains/d2.txt.json\n",
      "Biomechanical Attributes\n",
      "\n",
      "The main functions of the spine are biomechanical: to protect the spinal c\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(f\"Parsing {file}\")\n",
    "    store_content(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab1a517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batches</th>\n",
       "      <th>total</th>\n",
       "      <th>timeTaken</th>\n",
       "      <th>committedOperations</th>\n",
       "      <th>failedOperations</th>\n",
       "      <th>failedBatches</th>\n",
       "      <th>retries</th>\n",
       "      <th>errorMessages</th>\n",
       "      <th>batch</th>\n",
       "      <th>operations</th>\n",
       "      <th>wasTerminated</th>\n",
       "      <th>failedParams</th>\n",
       "      <th>updateStatistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>243</td>\n",
       "      <td>35</td>\n",
       "      <td>196</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>{'Cannot merge the following node because of n...</td>\n",
       "      <td>{'total': 243, 'committed': 196, 'failed': 47,...</td>\n",
       "      <td>{'total': 243, 'committed': 196, 'failed': 47,...</td>\n",
       "      <td>False</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'nodesDeleted': 0, 'labelsAdded': 114, 'relat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batches  total  timeTaken  committedOperations  failedOperations  \\\n",
       "0      243    243         35                  196                47   \n",
       "\n",
       "   failedBatches  retries                                      errorMessages  \\\n",
       "0             47        0  {'Cannot merge the following node because of n...   \n",
       "\n",
       "                                               batch  \\\n",
       "0  {'total': 243, 'committed': 196, 'failed': 47,...   \n",
       "\n",
       "                                          operations  wasTerminated  \\\n",
       "0  {'total': 243, 'committed': 196, 'failed': 47,...          False   \n",
       "\n",
       "  failedParams                                   updateStatistics  \n",
       "0           {}  {'nodesDeleted': 0, 'labelsAdded': 114, 'relat...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_query(\"\"\"\n",
    "CALL apoc.periodic.iterate(\"\n",
    "  MATCH (e:Entity)\n",
    "  WHERE e.id STARTS WITH 'Q'\n",
    "  RETURN e\n",
    "\",\"\n",
    "  // Prepare a SparQL query\n",
    "  WITH 'SELECT * WHERE{ ?item rdfs:label ?name . filter (?item = wd:' + e.id + ') filter (lang(?name) = \\\\\"en\\\\\") ' +\n",
    "     'OPTIONAL {?item wdt:P31 [rdfs:label ?label] .filter(lang(?label)=\\\\\"en\\\\\")}}' AS sparql, e\n",
    "  // make a request to Wikidata\n",
    "  CALL apoc.load.jsonParams(\n",
    "    'https://query.wikidata.org/sparql?query=' + \n",
    "      + apoc.text.urlencode(sparql),\n",
    "      { Accept: 'application/sparql-results+json'}, null)\n",
    "  YIELD value\n",
    "  UNWIND value['results']['bindings'] as row\n",
    "  SET e.wikipedia_name = row.name.value\n",
    "  WITH e, row.label.value AS label\n",
    "  MERGE (c:Class {id:label})\n",
    "  MERGE (e)-[:INSTANCE_OF]->(c)\n",
    "  RETURN distinct 'done'\", {batchSize:1, retry:1})\n",
    "\"\"\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e57c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723260f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
